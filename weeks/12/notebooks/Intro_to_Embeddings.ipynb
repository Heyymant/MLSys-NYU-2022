{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "underlying-payroll",
   "metadata": {},
   "source": [
    "# Intro to Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cd420",
   "metadata": {},
   "source": [
    "In this notebook, we learn on how to build good embeddings for words, through sequential data (sentences) - in other words, \"Word2Vec\". We are not so much interested in the NLP part of it, but in a more general point: if we pick a good prediction task (such as guessing the next word in a sentence), we will obtain \"good embeddings\", that is embeddings that are good representations of the underlying concepts.\n",
    "\n",
    "You are encouraged to play around with the code and modify / re-built parts of it as you fit: there is NO substitute for \"tinkering with code\" to understand how all the concepts fit together (corollary: all this code is written for pedagogical purposes, so some functions are re-used from previous lectures to provide a self-sufficient script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global imports\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from random import sample\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab0544",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the datasets library installed\n",
    "# see: https://github.com/huggingface/datasets\n",
    "\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff40c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# some utils function\n",
    "def get_finance_sentiment_dataset(split: str='sentences_allagree'):\n",
    "    # load financial dataset from HF\n",
    "    from datasets import load_dataset\n",
    "    # https://huggingface.co/datasets/financial_phrasebank\n",
    "    # by default, load just sentences for which all annotators agree\n",
    "    dataset = load_dataset(\"financial_phrasebank\", split)\n",
    "    \n",
    "    return dataset['train']\n",
    "\n",
    "\n",
    "def get_finance_sentences():\n",
    "    dataset = get_finance_sentiment_dataset()\n",
    "    cleaned_dataset = [[pre_process_sentence(_['sentence']), _['label']] for _ in dataset]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from finance dataset\\n\".format(len(cleaned_dataset)))\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def pre_process_sentence(sentence: str):\n",
    "    # this choices are VERY important. Here, we take a simplified \n",
    "    # view, remove the punctuations and just lower case everything\n",
    "    lower_sentence = sentence.lower()\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in lower_sentence if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_dataset = get_finance_sentences()\n",
    "# print out the first items in the dataset, to check the format\n",
    "finance_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences without label for vectorizer part\n",
    "finance_dataset_sentences = [_[0] for _ in finance_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-seller",
   "metadata": {},
   "source": [
    "## From words to embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d602a",
   "metadata": {},
   "source": [
    "Let us use word2vec to get vectors for words first, and document after. We will use a fantastic Python library, gensim: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim==4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(\n",
    "    sentences: list,\n",
    "    min_count: int = 2,\n",
    "    vector_size: int = 48,\n",
    "    window: int = 2,\n",
    "    epochs: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Sentences is a list of lists, where each list is composed by tokens in a sentence: e.g.\n",
    "    \n",
    "    [\n",
    "        ['the', 'cat', 'is', 'on' ...],\n",
    "        ['i', 'live', 'in', 'nyc', ...],\n",
    "        ....\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    model =  gensim.models.Word2Vec(sentences=sentences,\n",
    "                                    min_count=min_count,\n",
    "                                    vector_size=vector_size,\n",
    "                                    window=window,\n",
    "                                    epochs=epochs)\n",
    "    \n",
    "    # this is how many words we will have in the space\n",
    "    print(\"# words in the space: {}\".format(len(model.wv.index_to_key)))\n",
    "\n",
    "    # we return the space in a format that will allow us to do nice things afterwards ;-)    \n",
    "    return model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use nltk tokenizer to break up sentences and build a word2vec model\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(finance_dataset_sentences[0], '\\n\\n', word_tokenize(finance_dataset_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(_) for _ in finance_dataset_sentences]\n",
    "# debug \n",
    "tokenized_sentences[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a counter to get a sense of the lexicon\n",
    "word_counter = Counter([item for sent in tokenized_sentences for item in sent])\n",
    "word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ca10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = train_word2vec_model(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22492cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word now has one vector corresponding to it ->\n",
    "w2v_model['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca8664",
   "metadata": {},
   "source": [
    "Now that we have a vector space, let's find words similar to a given term..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['company', 'profit']:\n",
    "    print('\\n======>{}\\n'.format(w), w2v_model.similar_by_word(w, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467f3a2",
   "metadata": {},
   "source": [
    "To get a sense of what the vectors look like, we print them out in 2D using TSNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_by_category_with_lookup(title, \n",
    "                                         words, \n",
    "                                         word_to_target_cat,\n",
    "                                         results):\n",
    "    \"\"\"\n",
    "    Just a plotting routine\n",
    "    \"\"\"\n",
    "    \n",
    "    groups = {}\n",
    "    for word, target_cat in word_to_target_cat.items():\n",
    "        if word not in words:\n",
    "            continue\n",
    "\n",
    "        word_idx = words.index(word)\n",
    "        x = results[word_idx][0]\n",
    "        y = results[word_idx][1]\n",
    "        if target_cat in groups:\n",
    "            groups[target_cat]['x'].append(x)\n",
    "            groups[target_cat]['y'].append(y)\n",
    "        else:\n",
    "            groups[target_cat] = {\n",
    "                'x': [x], 'y': [y]\n",
    "                }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    for group, data in groups.items():\n",
    "        ax.scatter(data['x'], data['y'], \n",
    "                   alpha=0.1 if group == 0 else 0.8, \n",
    "                   edgecolors='none', \n",
    "                   s=25, \n",
    "                   marker='o',\n",
    "                   label=group)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_analysis(embeddings, perplexity=25, n_iter=1000):\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=n_iter)\n",
    "    return tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to known categories of interest\n",
    "\n",
    "# 0 is the generic category\n",
    "words = w2v_model.index_to_key\n",
    "print(len(words))\n",
    "words_to_category = {w: 0 for w in words}\n",
    "# manually pick some words to display\n",
    "for w in ['company', 'profit', 'investment', 'loss', 'margin', 'group']:\n",
    "    words_to_category[w] = 1\n",
    "for w in ['with', 'of', 'from', 'by', 'as']:\n",
    "    words_to_category[w] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [w2v_model[w] for w in words]\n",
    "tsne_results = tsne_analysis(embeddings)\n",
    "assert len(tsne_results) == len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_by_category_with_lookup('Finance trained word2vec', words, words_to_category, tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b393ff0",
   "metadata": {},
   "source": [
    "_Why the quality is not ideal?_\n",
    "\n",
    "Our dataset is very small, and word2vec works much better when large corpora are used. What if we trained the vectors on a much bigger dataset, say, Wikipedia?\n",
    "\n",
    "We don't have to, as we can donwload embeddings for words as gently pretrained by other researchers - let's do the same qualitative checks, but on Wikipedia embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia\n",
    "pre_trained_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# test it out\n",
    "for w in ['company', 'profit']:\n",
    "    print('\\n======>{}\\n'.format(w), pre_trained_model.similar_by_word(w, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[w for w in w2v_model.index_to_key if w in pre_trained_model]\n",
    "print(len(words))\n",
    "pre_trained_vectors = [pre_trained_model[w] for w in words]\n",
    "pre_trained_tsne_results = tsne_analysis(pre_trained_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6554dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_by_category_with_lookup('Wikipedia trained word2vec', \n",
    "                                     words, \n",
    "                                     words_to_category, \n",
    "                                     pre_trained_tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bded37",
   "metadata": {},
   "source": [
    "_Let's try some other categories on the full Wikipedia corpus!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a new mapping and manually pick some words to display\n",
    "wiki_words_to_category = {}\n",
    "for w in ['dog', 'cat', 'bunny', 'horse', 'snake']:\n",
    "    wiki_words_to_category[w] = 1\n",
    "for w in ['with', 'of', 'from', 'by', 'as']:\n",
    "    wiki_words_to_category[w] = 2\n",
    "for w in ['italy', 'france', 'spain', 'germany', 'portugal', 'china']:\n",
    "    wiki_words_to_category[w] = 3\n",
    "for w in ['rome', 'berlin', 'paris', 'milan', 'madrid']:\n",
    "    wiki_words_to_category[w] = 4\n",
    "for w in ['tennis', 'soccer', 'basketball', 'volleyball', 'football']:\n",
    "    wiki_words_to_category[w] = 5\n",
    "for w in ['dress', 'skirt', 'pants', 'trousers', 'shirt', 'denim']:\n",
    "    wiki_words_to_category[w] = 6\n",
    "for w in ['pasta', 'pizza', 'dumplings', 'ramen', 'sushi', 'burger']:\n",
    "    wiki_words_to_category[w] = 7\n",
    "for w in ['dollar', 'yen', 'euro', 'rupee']:\n",
    "    wiki_words_to_category[w] = 8\n",
    "    \n",
    "# limit to 10000 words to speed up TSNE    \n",
    "wiki_words = list(set(sample(pre_trained_model.index_to_key, 1000) + list(wiki_words_to_category.keys()))) \n",
    "for w in wiki_words:\n",
    "    if w not in wiki_words_to_category:\n",
    "        wiki_words_to_category[w] = 0\n",
    "\n",
    "# display\n",
    "wiki_pre_trained_vectors = [pre_trained_model[w] for w in wiki_words]\n",
    "wiki_pre_trained_tsne_results = tsne_analysis(wiki_pre_trained_vectors)\n",
    "plot_scatter_by_category_with_lookup('Wikipedia trained word2vec', \n",
    "                                     wiki_words, \n",
    "                                     wiki_words_to_category, \n",
    "                                     wiki_pre_trained_tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4390229",
   "metadata": {},
   "source": [
    "### Bonus: word2vec for analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47c4aa",
   "metadata": {},
   "source": [
    "A famous property of word2vec is the ability to capture analogical relations through the embedding space, such as for example:\n",
    "\n",
    "man : king = woman : ?\n",
    "\n",
    "We re-use a pre-trained model, trained on wikipedia, to show how analogies are encoded in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a model trained on twitter: https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html\n",
    "pre_trained_model = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee9489",
   "metadata": {},
   "source": [
    "_Note that most_similar is performing here a KNN on the vector space using cosine similarity_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021db603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model, worda, wordb, wordc):\n",
    "    result = model.most_similar(negative=[worda], \n",
    "                                positive=[wordb, wordc])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(pre_trained_model, 'king', 'man', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['australia', 'canada', 'germany', 'ireland', 'italy']\n",
    "foods = [analogy(pre_trained_model, 'us', 'hamburger', country) for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5818e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, f in zip(countries, foods):\n",
    "    print(c, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
