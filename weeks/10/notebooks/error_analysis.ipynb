{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "underlying-payroll",
   "metadata": {},
   "source": [
    "# Error analysis and (better) testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc4063",
   "metadata": {},
   "source": [
    "We train a text classification model on financial news (mostly as a black-box, focus is not on modelling), and then use the model to understand some best practices in error analysis and testing.\n",
    "\n",
    "You are encouraged to play around with the code and modify / re-built parts of it as you fit: there is NO substitute for \"tinkering with code\" to understand how all the concepts fit together (corollary: all this code is written for pedagogical purposes, so some functions are re-used from previous lectures to provide a self-sufficient script)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d719be9",
   "metadata": {},
   "source": [
    "_First, let's make sure we are running from the virtual env_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global import\n",
    "# we import specific libraries in due time\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe1e72",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c86446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the datasets library installed\n",
    "# see: https://github.com/huggingface/datasets\n",
    "\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# some utils function\n",
    "def get_finance_sentiment_dataset(split: str='sentences_allagree'):\n",
    "    # load financial dataset from HF\n",
    "    from datasets import load_dataset\n",
    "    # https://huggingface.co/datasets/financial_phrasebank\n",
    "    # by default, load just sentences for which all annotators agree\n",
    "    dataset = load_dataset(\"financial_phrasebank\", split)\n",
    "    \n",
    "    return dataset['train']\n",
    "\n",
    "\n",
    "def get_finance_sentences():\n",
    "    dataset = get_finance_sentiment_dataset()\n",
    "    cleaned_dataset = [[pre_process_sentence(_['sentence']), _['label']] for _ in dataset]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from finance dataset\\n\".format(len(cleaned_dataset)))\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def pre_process_sentence(sentence: str):\n",
    "    # this choices are VERY important. Here, we take a simplified \n",
    "    # view, remove the punctuations and just lower case everything\n",
    "    lower_sentence = sentence.lower()\n",
    "    # remove punctuation\n",
    "    # nice suggestion from https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "    # if we change the exclude set, we can control what to exclude\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in lower_sentence if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_dataset = get_finance_sentences()\n",
    "# print out the first item in the dataset, to check the format\n",
    "finance_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ab27a",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Training a simple tf-idf classifier on text and return the model for analysis, prediction. More info about the model:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Some more details on NLP classification can be found for example in our 2021 course: https://github.com/jacopotagliabue/FREE_7773\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ed582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the final dataset splits etc.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "finance_dataset_text = [_[0] for _ in finance_dataset]\n",
    "finance_dataset_label = [_[1] for _ in finance_dataset]\n",
    "all_labels = set(finance_dataset_label)\n",
    "print(\"All labels are: {}\".format(all_labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(finance_dataset_text, \n",
    "                                                    finance_dataset_label, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "print(\"Total train examples {}, test {}\".format(len(X_train), len(y_train)))\n",
    "# debug with examples\n",
    "print(X_train[:2], y_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee714e",
   "metadata": {},
   "source": [
    "*NOTE: LABELS ARE 0 NEG, 1 NEUTRAL AND 2 POSITIVE !*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(tf_idf_x_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a simple classifier over the text vectors.\n",
    "    Model from https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    \"\"\"\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    model = MultinomialNB()\n",
    "    model.fit(tf_idf_x_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_trained_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Encapsulate the training here, as we really don't care about training details:\n",
    "    the model is just useful to discuss testing strategies!\n",
    "    \"\"\"\n",
    "    \n",
    "    # map text to numerical vectors using TF-IDF and some sensible defaults\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "    tfidf_train = vectorizer.fit_transform(X_train)\n",
    "    # debug: what does this shape mean?\n",
    "    print(tfidf_train.shape)\n",
    "    # train the model\n",
    "    model = train_model(tfidf_train, y_train)\n",
    "        \n",
    "    # since we are treating of all this \n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer, clf_model = get_trained_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360936e4",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "How well are we doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967add6b",
   "metadata": {},
   "source": [
    "First, we now instantiate a classifier, train it and then predicting unseen test cases as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = tf_vectorizer.transform(X_test)\n",
    "predicted = clf_model.predict(X_test_transformed)\n",
    "predicted_prob = clf_model.predict_proba(X_test_transformed)\n",
    "# debug output\n",
    "print(predicted[:2], predicted_prob[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a6e4b",
   "metadata": {},
   "source": [
    "We start by using standard quantitative metrics to evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def calculate_confusion_matrix_and_report(y_predicted, y_golden, with_plot=True):\n",
    "    # calculate confusion matrix: \n",
    "    cm = confusion_matrix(y_golden, y_predicted)\n",
    "    # build a readable report;\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_golden, y_predicted))\n",
    "    # plot the matrix\n",
    "    if with_plot:\n",
    "        plot_confusion_matrix(cm)\n",
    "                                          \n",
    "    return\n",
    "                                          \n",
    "def plot_confusion_matrix(c_matrix):\n",
    "    plt.imshow(c_matrix, cmap=plt.cm.Blues)\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-drive",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_test) == len(predicted)\n",
    "# manual inspection\n",
    "mistakes = [(x, p, y, prob) for x, p, y, prob in zip(X_test, predicted, y_test, predicted_prob) if p != y]\n",
    "print(\"Total of mistakes: {}\".format(len(mistakes)))\n",
    "# debug\n",
    "print(\"Sentence: {}\\nPredicted: {}, but it was: {}\\nProbs: {}\".format(*mistakes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    rnd_mistake = choice(mistakes)\n",
    "    print(\"Sentence: {}\\nPredicted: {}, but it was: {}\\nProbs: {}\\n=======\\n\".format(*rnd_mistake))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfa4f3",
   "metadata": {},
   "source": [
    "Now, let's run evaluation *per slice*: instead of considering the performances on all dataset, we split it according to categories important for our use case.\n",
    "\n",
    "In our example, we will assume we are interested in being accurate across all quarters, so we now report our metrics per slice / quarter.\n",
    "\n",
    "You can imagine many more relevant slices: report by industry (pharma, tech, etc.), market cap (over a 1B, over 100 etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we slice queries by quarter\n",
    "slices = {\n",
    "    \"first quarter\": [[], []],\n",
    "    \"second quarter\": [[], []],\n",
    "    \"third quarter\": [[], []],\n",
    "    \"fourth quarter\": [[], []]\n",
    "}\n",
    "\n",
    "for x, p, y in zip(X_test, predicted, y_test):\n",
    "    for _s in slices.keys():\n",
    "        if _s in x:\n",
    "            slices[_s][0].append(p)\n",
    "            slices[_s][1].append(y)\n",
    "            \n",
    "for _slice, test_set in slices.items():\n",
    "    if test_set[0]:\n",
    "        print(\"Total of # {} cases in slice: {}\".format(len(test_set[0]), _slice))\n",
    "        calculate_confusion_matrix_and_report(test_set[0], test_set[1], with_plot=False)\n",
    "        print(\"\\n===========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d040753",
   "metadata": {},
   "source": [
    "## Black-box testing\n",
    "\n",
    "Sometime, we wish to test our models on edge cases not present in the test set, or on specific conditions we know are important to safely deploy it: for example, if we work for company X, we may want to \"double check\" the behavior of the system in carefully crafted stories about company X, to make sure the model WOULD behave correctly, if presented with those cases.\n",
    "\n",
    "More specifically, we will adapt the “black box testing” from traditional software systems to ML systems: it should be possible to evaluate the performance of a complex system by treating it as a black box, and only supply input-output pairs that are relevant for our qualitative understanding (see for example the excellent paper: https://arxiv.org/abs/2005.04118)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a28720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for edge cases / interesting cases / regression errors\n",
    "\n",
    "# CASE 1:\n",
    "# I'm particularly interesting in some company, say\n",
    "# https://en.wikipedia.org/wiki/Comptel\n",
    "# and want to make sure we are doing well there!\n",
    "\n",
    "companies_I_care_about = ['comptel']\n",
    "\n",
    "for company in companies_I_care_about:\n",
    "    print(\"\\n======\\nFocus on target company: {}\\n\".format(company))\n",
    "    for x, p, y in zip(X_test, predicted, y_test):\n",
    "        if company in x:\n",
    "            print(\"For '{}' =>\\ngolden {}, predicted {}\\n\".format(\n",
    "                x,\n",
    "                y, \n",
    "                p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASE 2:\n",
    "# Assuming we have some specific sentences to monitor, let's check for that!\n",
    "sentences_I_care_about = [\n",
    "    'the company slipped to an operating loss of eur 26 million from a profit of eur 13 million',\n",
    "    'revenue in the quarter fell 8 percent to  euro  24 billion compared to a year earlier'\n",
    "]\n",
    "labels_I_care_about = [0, 0]\n",
    "\n",
    "for x, p in zip(X_test, predicted):\n",
    "     if x.strip() in sentences_I_care_about:\n",
    "        print(\"For '{}', I expect {}, it was {}\\n\".format(\n",
    "            x,\n",
    "            p, \n",
    "            labels_I_care_about[sentences_I_care_about.index(x.strip())]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96275a4",
   "metadata": {},
   "source": [
    "*BONUS: test for robustness*\n",
    "\n",
    "Ideally, a model should perform the same when input change in a small way (check this paper on \"adversarial attacks\": https://arxiv.org/pdf/1412.6572.pdf!).\n",
    "\n",
    "For example, in test classification, we desire to have a system robust to alternative specifications of the text, i.e. we expect the response to the pair (\"revenue in the quarter fell 8 percent to  euro  24 billion compared to a year earlier\", \"revenue in the quarter diminished by 8 percent to  euro  24 billion compared to the previous year\") to be identical.\n",
    "\n",
    "While a full-fledge treatment of this problem is out of scope, let's see how this intuition works with working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for perturbations\n",
    "test_sentences = [\n",
    "    'the company slipped to an operating loss of eur 26 million from a profit of eur 13 million',\n",
    "    'revenue in the quarter fell 8 percent to  euro  24 billion compared to a year earlier'\n",
    "]\n",
    "\n",
    "perturbated_sentences = [\n",
    "    'operating loss surged to eur 26 million from a profit of eur 13 million',\n",
    "    'revenue in the quarter diminished by 8 percent to  euro  24 billion compared to the previous year'\n",
    "]\n",
    "\n",
    "test_predicted = clf_model.predict(tf_vectorizer.transform(test_sentences))\n",
    "perturbated_predicted = clf_model.predict(tf_vectorizer.transform(perturbated_sentences))\n",
    "\n",
    "for s, t, p in zip(test_sentences, test_predicted, perturbated_predicted):\n",
    "    print(\"\\nFor sentence '{}', prediction was: {}, under perturbation: {}\".format(\n",
    "        s, t, p\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc58715",
   "metadata": {},
   "source": [
    "How do we scale this to more and more examples? A quick and easy way to generate perturbation is called back-translation.\n",
    "\n",
    "The idea is that you can use machine translation to go:\n",
    "\n",
    "SOURCE -> TARGET -> NEW_SOURCE\n",
    "\n",
    "where NEW_SOURCE is a semantically equivalent, but not identical version of source. For example:\n",
    "\n",
    "'hi' -> Italian target: 'ciao' -> 'hello'\n",
    "\n",
    "'hi' and 'hello' have the same meaning so, 'hello' may be considered a perturbation of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install BackTranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is really small hack-y example!!!\n",
    "from BackTranslation import BackTranslation\n",
    "trans = BackTranslation(url=[\n",
    "      'translate.google.com',\n",
    "      'translate.google.co.kr',\n",
    "    ])\n",
    "for t in test_sentences:\n",
    "    result = trans.translate(t, src='en', tmp = 'zh-cn')\n",
    "    print(\"Original is: {}\\nNew sentence is: {}\\n\\n\".format(t, result.result_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db4d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e9a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
